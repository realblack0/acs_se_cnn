{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from acs_se_cnn.model import SEBlock, ACSLayer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.functional import F\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "###################\n",
    "## Configuration ##\n",
    "###################\n",
    "# SYSTEM\n",
    "class Args:\n",
    "    name = \"baseline\"\n",
    "    device = \"cpu\"\n",
    "    subject = 1\n",
    "args = Args()\n",
    "    \n",
    "device = torch.device(args.device)\n",
    "\n",
    "# LEARNING STRATEGY\n",
    "batch_size = 20\n",
    "epochs     = 500\n",
    "criterion    = nn.BCEWithLogitsLoss()\n",
    "Optimizer    = torch.optim.RMSprop\n",
    "lr              = 0.001\n",
    "# HYPER PARAMETER\n",
    "sparse_lambda = 1 # ?\n",
    "\n",
    "fit_data = \"2a\"\n",
    "data_path = \"cwt_data/2a\" if fit_data==\"2a\" else \"cwt_data/2b\"\n",
    "\n",
    "###################\n",
    "#### Modeling #####\n",
    "###################\n",
    "\n",
    "class JHModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        # MODEL HYPER PARAMETER ,\n",
    "        n_channels = 22 if fit_data==\"2a\" else 3,\n",
    "        n_kerenls  = 64,\n",
    "        r          = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ATUO CHANNEL SELECTION\n",
    "        self.acs_layer   = ACSLayer(c=n_channels, r=r)\n",
    "        \n",
    "        # FEATURE EXTRACTION\n",
    "        self.conv_layer1 = nn.Conv2d(n_channels, n_kerenls, kernel_size=(4,4), stride=(2, 2), padding=1)\n",
    "        self.se_block1   = SEBlock(c=n_kerenls, r=r)\n",
    "        \n",
    "        self.conv_layer2 = nn.Conv2d(n_kerenls, n_kerenls, kernel_size=(4,4), stride=(4, 4))\n",
    "        self.se_block2   = SEBlock(c=n_kerenls, r=r)\n",
    "        \n",
    "        self.conv_layer3 = nn.Conv2d(n_kerenls, n_kerenls, kernel_size=(4,4), stride=(4, 4))\n",
    "        self.se_block3   = SEBlock(c=n_kerenls, r=r)\n",
    "        \n",
    "        # OUTPUT\n",
    "        self.fc1 = nn.Linear(4, 1)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "#         self.sigmoid = F.sigmoid()\n",
    "        \n",
    "    def forward(self, inputs, return_s_acs=False): \n",
    "        \"\"\" \n",
    "        Args\n",
    "        ----\n",
    "            inputs (batch, channel, height, width) \n",
    "        \"\"\"\n",
    "        # ATUO CHANNEL SELECTION\n",
    "        x, s_acs = self.acs_layer(inputs)\n",
    "#         B, _, _, _ = inputs.shape\n",
    "#         s_acs = inputs.new_zeros(B,22,1,1)\n",
    "        \n",
    "        # FEATURE EXTRACTION\n",
    "        x = self.conv_layer1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.se_block1(x)\n",
    "        \n",
    "        x = self.conv_layer2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.se_block2(x)\n",
    "        \n",
    "        x = self.conv_layer3(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.se_block3(x)        \n",
    "        \n",
    "        # OUTPUT\n",
    "        B, _, _, _ = x.shape\n",
    "        x = x.reshape(B, 64, 4)\n",
    "        x = self.fc1(x) # (B, 64, 1)\n",
    "        x = x.squeeze() # (B, 64)\n",
    "        x = F.elu(x)\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        if return_s_acs:\n",
    "            return out, s_acs\n",
    "        else:\n",
    "            return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams:\n",
    "    # for train\n",
    "    # batch_size = 2\n",
    "    per_batch = 20\n",
    "    epoch = 500\n",
    "\n",
    "    # for model\n",
    "    input_channel = 22\n",
    "    output_channel = 64\n",
    "    r = 2\n",
    "    \n",
    "hparams = HParams()\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, input_channel):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_channel, int(input_channel/hparams.r), bias=False)\n",
    "        self.fc2 = torch.nn.Linear(int(input_channel/hparams.r), input_channel, bias=False)\n",
    "        self.sm = torch.nn.Sigmoid()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x -> (B, C, H, W)\n",
    "        _, _, H, W = x.shape\n",
    "        output = x.clone()\n",
    "        x = torch.sum(x, axis=(2,3)) / (H*W)\n",
    "        x = x.squeeze()\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sm(x)\n",
    "        x = x.unsqueeze(2).unsqueeze(3)\n",
    "        return output * x, x\n",
    "\n",
    "\n",
    "class Recognizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Recognizer, self).__init__()\n",
    "        self.acs = SqueezeExcitation(hparams.input_channel)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(hparams.input_channel,\n",
    "                                     hparams.output_channel,\n",
    "                                     kernel_size=4,\n",
    "                                     stride=2,\n",
    "                                     padding=1)\n",
    "        self.elu1 = torch.nn.ELU()\n",
    "        self.se1 = SqueezeExcitation(hparams.output_channel)\n",
    "        self.conv2 = torch.nn.Conv2d(hparams.output_channel,\n",
    "                                     hparams.output_channel,\n",
    "                                     kernel_size=4,\n",
    "                                     stride=4)\n",
    "        self.elu2 = torch.nn.ELU()\n",
    "        self.se2 = SqueezeExcitation(hparams.output_channel)\n",
    "        self.conv3 = torch.nn.Conv2d(hparams.output_channel,\n",
    "                                     hparams.output_channel,\n",
    "                                     kernel_size=4,\n",
    "                                     stride=4)\n",
    "        self.elu3 = torch.nn.ELU()\n",
    "        self.se3 = SqueezeExcitation(hparams.output_channel)\n",
    "        self.fc = torch.nn.Linear(4,1)\n",
    "        self.elu4 = torch.nn.ELU()\n",
    "        self.ffc = torch.nn.Linear(64,1)\n",
    "        self.sm = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x -> (B, C, H, W)\n",
    "        B, _, _, _ = x.shape\n",
    "        x, sp = self.acs(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.elu1(x)\n",
    "        x, _ = self.se1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.elu2(x)\n",
    "        x, _ = self.se2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.elu3(x)\n",
    "        x, _ = self.se3(x)\n",
    "        # x -> (B, C, 2, 2)\n",
    "        x = x.reshape(B,-1,4)\n",
    "        x = self.fc(x).squeeze()\n",
    "        x = self.elu4(x)\n",
    "        x = self.ffc(x)\n",
    "        # x -> (B, C, 1)\n",
    "        return sp, self.sm(x).squeeze()\n",
    "\n",
    "class SWModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SWModel, self).__init__()\n",
    "        self.recognizer = Recognizer()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, _, _ = x.shape\n",
    "        sparse, bce = self.recognizer(x)\n",
    "        sparse_loss = torch.norm(sparse, 1) / B\n",
    "        return bce, sparse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "jh_model = JHModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "sw_model = SWModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jh acs_layer.excitation.0.weight\n",
      "sw recognizer.acs.fc1.weight\n",
      "\tAre they same? tensor(True)\n",
      "\n",
      "jh acs_layer.excitation.2.weight\n",
      "sw recognizer.acs.fc2.weight\n",
      "\tAre they same? tensor(True)\n",
      "\n",
      "jh conv_layer1.weight\n",
      "sw recognizer.conv1.weight\n",
      "\tAre they same? tensor(True)\n",
      "\n",
      "jh conv_layer1.bias\n",
      "sw recognizer.conv1.bias\n",
      "\tAre they same? tensor(True)\n",
      "\n",
      "jh se_block1.excitation.0.weight\n",
      "sw recognizer.se1.fc1.weight\n",
      "\tAre they same? tensor(True)\n",
      "\n",
      "jh se_block1.excitation.2.weight\n",
      "sw recognizer.se1.fc2.weight\n",
      "\tAre they same? tensor(True)\n",
      "\n",
      "jh conv_layer2.weight\n",
      "sw recognizer.conv2.weight\n",
      "\tAre they same? tensor(True)\n",
      "\n",
      "jh conv_layer2.bias\n",
      "sw recognizer.conv2.bias\n",
      "\tAre they same? tensor(True)\n",
      "\n",
      "jh se_block2.excitation.0.weight\n",
      "sw recognizer.se2.fc1.weight\n",
      "\tAre they same? tensor(True)\n",
      "\n",
      "jh se_block2.excitation.2.weight\n",
      "sw recognizer.se2.fc2.weight\n",
      "\tAre they same? tensor(True)\n",
      "\n",
      "jh conv_layer3.weight\n",
      "sw recognizer.conv3.weight\n",
      "\tAre they same? tensor(True)\n",
      "\n",
      "jh conv_layer3.bias\n",
      "sw recognizer.conv3.bias\n",
      "\tAre they same? tensor(True)\n",
      "\n",
      "jh se_block3.excitation.0.weight\n",
      "sw recognizer.se3.fc1.weight\n",
      "\tAre they same? tensor(True)\n",
      "\n",
      "jh se_block3.excitation.2.weight\n",
      "sw recognizer.se3.fc2.weight\n",
      "\tAre they same? tensor(True)\n",
      "\n",
      "jh fc1.weight\n",
      "sw recognizer.fc.weight\n",
      "\tAre they same? tensor(True)\n",
      "\n",
      "jh fc1.bias\n",
      "sw recognizer.fc.bias\n",
      "\tAre they same? tensor(True)\n",
      "\n",
      "jh fc2.weight\n",
      "sw recognizer.ffc.weight\n",
      "\tAre they same? tensor(True)\n",
      "\n",
      "jh fc2.bias\n",
      "sw recognizer.ffc.bias\n",
      "\tAre they same? tensor(True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for jh, sw in zip(jh_model.named_parameters(), sw_model.named_parameters()):\n",
    "    print(\"jh\", jh[0])\n",
    "    print(\"sw\", sw[0])\n",
    "    print(\"\\tAre they same?\", torch.all(jh[1]==sw[1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
